// D:\Projects\NextMove\app.py
from fastapi import FastAPI, Response
from pydantic import BaseModel
from typing import Optional, Dict, Any
import json
import uvicorn

# --- Import Pipelines ---
from pipelines.query_analyzer_test_pipeline import run_single_query
from pipelines.query_decomposer_test_pipeline import decompose_single_query
from pipelines.run_pipeline import run_pipeline

app = FastAPI(title="NextMove Query Processing API")

# ---------------------
# üì¶ Request Model
# ---------------------
class QueryRequest(BaseModel):
    query: str
    debug_mode: Optional[bool] = False 
    use_history: Optional[bool] = False
    session_id: Optional[str] = "default_session" # <--- Added Session ID

# ---------------------
# üì¶ Response Models
# ---------------------
class AnalyzeResponse(BaseModel):
    analyzed_result: Dict[str, Any]

class DecomposeResponse(BaseModel):
    analyzed_result: Dict[str, Any]
    decomposed_result: Dict[str, Any]

class RunResponse(BaseModel):
    final_answer: str
    debug_info: Optional[Dict[str, Any]] = None

class ErrorResponse(BaseModel):
    error: str

# ---------------------
# üîç Analyze Endpoint (Test)
# ---------------------
@app.post("/analyze", response_model=AnalyzeResponse, responses={500: {"model": ErrorResponse}})
def analyze_query(request: QueryRequest):
    # Note: Simple test endpoint, does not use history context
    result = run_single_query(request.query)
    if result is None:
        return Response(content=json.dumps({"error": "Failed to analyze the query"}), status_code=500, media_type="application/json")
    return {"analyzed_result": result}

# ---------------------
# üî® Decompose Endpoint (Test)
# ---------------------
@app.post("/decompose", response_model=DecomposeResponse, responses={500: {"model": ErrorResponse}})
def decompose_query(request: QueryRequest):
    analyzed_result = run_single_query(request.query)
    if analyzed_result is None:
        return Response(content=json.dumps({"error": "Failed to analyze the query"}), status_code=500, media_type="application/json")

    analyzed_result["original_query"] = request.query
    try:
        decomposed_result = decompose_single_query(analyzed_result)
        return {
            "analyzed_result": analyzed_result,
            "decomposed_result": decomposed_result
        }
    except Exception as e:
        return Response(content=json.dumps({
            "analyzed_result": analyzed_result,
            "error": f"Failed to decompose: {str(e)}"
        }), status_code=500, media_type="application/json")

# ---------------------
# üîÅ Full Pipeline Endpoint (Main)
# ---------------------
@app.post("/run", response_model=RunResponse, responses={500: {"model": ErrorResponse}})
def run_full_pipeline_endpoint(request: QueryRequest):
    try:
        # Pass all parameters including session_id to the pipeline
        pipeline_response = run_pipeline(
            natural_language_query=request.query,
            debug_mode=request.debug_mode,
            use_history=request.use_history,
            session_id=request.session_id 
        )

        if pipeline_response is None:
            return Response(content=json.dumps({"error": "Pipeline execution returned None"}), status_code=500, media_type="application/json")
        
        return pipeline_response

    except Exception as e:
        return Response(content=json.dumps({"error": f"Internal Server Error: {str(e)}"}), status_code=500, media_type="application/json")

# ---------------------
# üåê Root Endpoint
# ---------------------
@app.get("/")
def root():
    return {
        "message": "Welcome to NextMove API. Use /run for the full pipeline."
    }

# ---------------------
# üöÄ Run with Uvicorn
# ---------------------
if __name__ == "__main__":
    uvicorn.run("app:app", host="127.0.0.1", port=8000, reload=True)

// D:\Projects\NextMove\chatbot_app.py
import streamlit as st
import requests
import json
import time
import uuid

# --- Page Configuration ---
st.set_page_config(
    page_title="NextMove Job Chatbot",
    page_icon="ü§ñ",
    layout="wide"
)

# --- 1. INITIALIZATION (Must be at the top) ---
# Initialize Session ID if missing
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

# Initialize Chat Messages if missing
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- API Endpoint ---
FASTAPI_ENDPOINT = "http://127.0.0.1:8000/run"

# --- Page Title ---
st.title("ü§ñ NextMove Job Chatbot")

# --- Sidebar & Settings ---
with st.sidebar:
    st.header("Settings")
    
    debug_mode = st.checkbox("üõ†Ô∏è Debug Mode", value=False, help="Show intermediate steps (SQL, JSON, etc.)")
    use_history = st.checkbox("üß† History Aware", value=True, help="Enable persistent memory.")
    
    st.caption(f"Session ID: {st.session_state.session_id[:8]}...") 
    
    st.divider()
    
    if st.button("üóëÔ∏è Clear Chat & Reset Session"):
        # Clear local UI history
        st.session_state.messages = []
        # Generate new session ID (simulate fresh start)
        st.session_state.session_id = str(uuid.uuid4())
        # Rerun immediately to reflect empty state
        st.rerun()

# --- Display Past Messages ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "debug_info" in message:
            with st.expander("Show Debug Info"):
                st.json(message["debug_info"])

# --- Handle User Input ---
if prompt := st.chat_input("Ask about job postings..."):
    # 1. Append user message to state
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # 2. Display user message immediately
    with st.chat_message("user"):
        st.markdown(prompt)

    # 3. Call Backend
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("Thinking... ‚è≥")
        start_time = time.time()
        
        try:
            # Prepare payload
            payload = {
                "query": prompt, 
                "debug_mode": debug_mode,
                "use_history": use_history,
                "session_id": st.session_state.session_id
            }
            
            # Send Request
            response = requests.post(FASTAPI_ENDPOINT, json=payload, timeout=300)
            response.raise_for_status()
            
            # Parse Response
            data = response.json()
            final_answer = data.get("final_answer", "Sorry, I received an invalid response.")
            debug_info = data.get("debug_info")
            
            # Display Answer
            message_placeholder.markdown(final_answer)

            # Store Assistant Response
            assistant_message = {"role": "assistant", "content": final_answer}
            
            if debug_mode and debug_info:
                assistant_message["debug_info"] = debug_info
                with st.expander("Show Debug Info"):
                    st.json(debug_info)
            
            st.session_state.messages.append(assistant_message)

        except requests.exceptions.ConnectionError:
            message_placeholder.error("Connection Error: Is the FastAPI server running?")
        except Exception as e:
            message_placeholder.error(f"An error occurred: {e}")

// D:\Projects\NextMove\pipelines\run_pipeline.py
# D:\Projects\NextMove\pipelines\run_pipeline.py

import json
from typing import Dict, Any
from components.analyzer_and_decomposer.query_analyzer import query_analyze
from pipelines.query_decomposer_test_pipeline import decompose_single_query
from components.connectors.mysql_connector import MySQLConnector
from components.synthesizer.result_synthesizer import synthesize_results
from components.history_manager.history_handler import HistoryHandler
from components.learner.graph_learner import GraphLearner 
from components.synthesizer.integration import ResultIntegrator 

from entities.config import (
    LINKEDIN_DB_HOST, LINKEDIN_DB_USER, LINKEDIN_DB_PASSWORD, LINKEDIN_DB_NAME,
    NAUKRI_DB_HOST, NAUKRI_DB_USER, NAUKRI_DB_PASSWORD, NAUKRI_DB_NAME
)

DB_CONNECTION_TIMEOUT = 3

def run_pipeline(
    natural_language_query: str, 
    debug_mode: bool = False,
    use_history: bool = False,
    session_id: str = "default_session"
) -> Dict[str, Any]:
    """
    Executes the full NextMove pipeline with Integration, Ranking, and Active Learning.
    """
    print(f"=== NextMove Pipeline Started (Session: {session_id}) ===\n")

    # --- 0. Context Management ---
    chat_context = ""
    history_handler = None

    if use_history:
        print("[INFO] History Aware Mode: ON. Loading context...")
        try:
            history_handler = HistoryHandler(session_id=session_id)
            chat_context = history_handler.get_context_string()
        except Exception as e:
            print(f"[WARN] Failed to load history: {e}. Proceeding without context.")
    else:
        print("[INFO] History Aware Mode: OFF.")

    # --- Step 1: Analyze Query ---
    print("[STEP 1] Analyzing query...")
    analyzed_result = query_analyze(natural_language_query, chat_history_context=chat_context)
    
    if analyzed_result is None:
        return {
            "final_answer": "I'm sorry, I wasn't able to understand your query.",
            "debug_info": {"error": "Query analysis failed"}
        }

    user_intent = analyzed_result.get("user_intent", natural_language_query)
    unstructured_query = analyzed_result.get("unstructured_query", "")
    global_sql = analyzed_result.get("sql_query")

    print(f"   > Resolved Intent: {user_intent}")

    federated_queries = {}
    db_results = {}
    
    # --- Step 2 & 3: Decompose & Execute ---
    if global_sql:
        print("\n[INFO] SQL query found. Running decomposition and execution...")
        
        # Step 2: Decomposition
        print("\n[STEP 2] Decomposing analyzed query...")
        federated_queries = decompose_single_query(analyzed_result)
        structured_queries = federated_queries.get("structured", {})

        # Step 3: Execution
        print("\n[STEP 3] Executing structured queries on data sources...\n")

        def run_mysql(host, user, pwd, db, sql, source):
            if not sql: return "No query generated."
            try:
                conn = MySQLConnector(host, user, pwd, db, timeout=DB_CONNECTION_TIMEOUT)
                conn.connect()
                rows = conn.execute_query_as_dict(sql)
                conn.disconnect()
                return rows
            except Exception as e:
                # We return a dict with 'error' key to track failures per source
                return {"error": f"Failed: {e}"}

        db_results["Linkedin_source"] = run_mysql(
            LINKEDIN_DB_HOST, LINKEDIN_DB_USER, LINKEDIN_DB_PASSWORD, LINKEDIN_DB_NAME,
            structured_queries.get("Linkedin_source"), "Linkedin"
        )
        
        db_results["Naukri_source"] = run_mysql(
            NAUKRI_DB_HOST, NAUKRI_DB_USER, NAUKRI_DB_PASSWORD, NAUKRI_DB_NAME,
            structured_queries.get("Naukri_source"), "Naukri"
        )

        # --- STEP 3.5: Integration & Ranking (Updated for Robustness) ---
        print("[INFO] Integrating and Ranking results...")
        try:
            integrator = ResultIntegrator()
            
            # 1. Isolate valid job lists (ignore errors for calculation)
            valid_job_lists = {k: v for k, v in db_results.items() if isinstance(v, list)}
            
            # 2. Determine Limit
            limit = analyzed_result.get("limit", 10)
            
            # 3. Run Integration Logic
            if valid_job_lists:
                top_k_jobs = integrator.integrate_and_rank(
                    results_dict=valid_job_lists,
                    user_intent=user_intent,
                    limit=limit
                )
            else:
                top_k_jobs = []

            # 4. ROBUST OUTPUT CONSTRUCTION
            # If we found jobs, we ONLY send the jobs to the LLM. 
            # We suppress errors from specific sources so the LLM focuses on the data we found.
            if top_k_jobs:
                print(f"[INFO] Success: Found {len(top_k_jobs)} jobs. Suppressing partial errors.")
                db_results = {"Top_Ranked_Jobs": top_k_jobs}
            else:
                # [FIX] Keep BOTH errors and empty lists so we can see Naukri's status
                print("[WARN] No jobs found. Some sources failed, others returned 0.") # <--- Look for this message in your logs!
                
                errors = {k: v for k, v in db_results.items() if isinstance(v, dict) and "error" in v}
                empty_successes = {k: v for k, v in db_results.items() if isinstance(v, list) and not v}
                
                db_results = {**errors, **empty_successes}
                
                if errors:
                    print("[WARN] No jobs found due to DB errors.")
                    db_results = errors # Pass errors to LLM
                else:
                    print("[INFO] Query ran successfully but returned 0 results.")
                    db_results = {"Top_Ranked_Jobs": []} # Empty list implies valid search, just no matches

        except Exception as e:
            print(f"[WARN] Integration/Ranking failed: {e}. Using raw results.")
            # In a catastrophic integration fail, we fall back to whatever db_results we had
        # ------------------------------------------------

        # --- PHASE 2: ACTIVE LEARNING ---
        try:
            # Fire the learner to update the graph based on what we found
            learner = GraphLearner()
            # We pass the raw db_results or integrated ones; learner handles format check
            learner.learn_from_results(user_intent, db_results)
        except Exception as e:
            print(f"[WARN] Learning step skipped: {e}")
        # --------------------------------------

    else:
        print("\n[INFO] No SQL query detected. Bypassing Steps 2 & 3.")
        federated_queries = {"info": "Bypassed: General knowledge query."}
        db_results = {"info": "Bypassed: No SQL executed."}

    # --- Step 4: Synthesize ---
    print("[STEP 4] Synthesizing final answer...")
    synthesis_response = synthesize_results(
        natural_language_query=natural_language_query,
        unstructured_query=unstructured_query,
        database_results=db_results,
        user_intent=user_intent
    )
    
    final_answer = synthesis_response["final_answer"]
    final_llm_prompts = synthesis_response["prompts_used"]

    # --- Step 5: History Update ---
    if use_history and history_handler:
        try:
            history_handler.add_interaction(natural_language_query, final_answer)
        except Exception as e:
            print(f"[ERROR] Failed to save history: {e}")

    print("\n=== Pipeline Completed ===")
    
    # Return Final Answer & Debug Info
    if debug_mode:
        return {
            "final_answer": final_answer,
            "debug_info": {
                "session_id": session_id,
                "0_history_context": chat_context,
                "1_query_analysis": analyzed_result,
                "2_query_decomposition": federated_queries,
                "3_database_results": db_results,
                "4_final_llm_prompts": final_llm_prompts
            }
        }
    else:
        return {
            "final_answer": final_answer,
            "debug_info": None
        }

// D:\Projects\NextMove\pipelines\query_analyzer_test_pipeline.py
import os
import json
from constants import QUERY_INPUT_FILE_PATH, QUERY_ANALYZE_OUTPUT_FILE_PATH
from components.analyzer_and_decomposer.query_analyzer import query_analyze


def analyze_all_queries():
    """
    Reads all queries from a file, analyzes them using the query analyzer,
    saves results to a .jsonl file, and prints output per query.
    """
    if not os.path.exists(QUERY_INPUT_FILE_PATH):
        print(f"[ERROR] Query file not found: {QUERY_INPUT_FILE_PATH}")
        return

    with open(QUERY_INPUT_FILE_PATH, "r", encoding="utf-8") as file:
        queries = [line.strip() for line in file if line.strip()]

    print(f"\nFound {len(queries)} queries in file.\n")

    os.makedirs(os.path.dirname(QUERY_ANALYZE_OUTPUT_FILE_PATH), exist_ok=True)
    with open(QUERY_ANALYZE_OUTPUT_FILE_PATH, "w", encoding="utf-8") as outfile:
        for i, query in enumerate(queries, start=1):
            print(f"\n--- Query {i} ---")
            print(f"Input: {query}")
            try:
                result = query_analyze(query)
                print("Output:")
                print(json.dumps(result, indent=2))

                # Save to output file
                record = {
                    "query": query,
                    "result": result
                }
                outfile.write(json.dumps(record) + "\n")

            except Exception as e:
                print(f"Error analyzing query: {e}")
                error_record = {
                    "query": query,
                    "error": str(e)
                }
                outfile.write(json.dumps(error_record) + "\n")
            print("-" * 50)

    print(f"\nResults saved to: {QUERY_ANALYZE_OUTPUT_FILE_PATH}")


def run_single_query(query: str):
    """
    Runs a single query through the analyzer and returns the result.
    """
    print(f"\nAnalyzing single query:\n{query}")
    try:
        result = query_analyze(query)
        return result
    except Exception as e:
        print(f"Error: {e}")
        return None


// D:\Projects\NextMove\pipelines\query_decomposer_test_pipeline.py
import os
import json
from typing import Dict
from constants import (
    QUERY_ANALYZE_OUTPUT_FILE_PATH,
    QUERY_DECOMPOSE_OUTPUT_FILE_PATH
)
from components.analyzer_and_decomposer.query_decomposer import prepare_federated_queries

# -----------------------------
# üîπ Decompose a single query
# -----------------------------
def decompose_single_query(analyzer_result: Dict) -> Dict:
    """
    Takes analyzer output (dict containing sql_query + unstructured_query),
    runs decomposition, translation, validation, and optional LLM retry.
    """
    return prepare_federated_queries(
        analyzed_result=analyzer_result,
        use_llm_retry=True,
    )


# -----------------------------
# üîπ Batch Decomposition from JSONL
# -----------------------------
def decompose_all_queries():
    """
    Reads the query_analyzer output .jsonl file, applies decomposition,
    and saves federated queries to another .jsonl file.
    """
    if not os.path.exists(QUERY_ANALYZE_OUTPUT_FILE_PATH):
        raise FileNotFoundError(f"Input file not found: {QUERY_ANALYZE_OUTPUT_FILE_PATH}")

    os.makedirs(os.path.dirname(QUERY_DECOMPOSE_OUTPUT_FILE_PATH), exist_ok=True)

    with open(QUERY_ANALYZE_OUTPUT_FILE_PATH, "r", encoding="utf-8") as infile, \
         open(QUERY_DECOMPOSE_OUTPUT_FILE_PATH, "w", encoding="utf-8") as outfile:

        for line_count, line in enumerate(infile, start=1):
            try:
                record = json.loads(line)
                query = record.get("query")
                result = record.get("result")

                if not result or not isinstance(result, dict):
                    raise ValueError("Missing or invalid 'result' in record.")

                # Attach original query for retry context
                result["original_query"] = query

                federated = decompose_single_query(result)

                output_record = {
                    "query": query,
                    "federated_query": federated
                }

                outfile.write(json.dumps(output_record) + "\n")

            except Exception as e:
                # Only essential error message
                print(f"‚ùå Failed to process line {line_count}: {e}")



// D:\Projects\NextMove\entities\config.py
import os
from dotenv import load_dotenv

# Use absolute path to .env file
dotenv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '.env'))
load_dotenv(dotenv_path=dotenv_path)

# ---------------------------
# Database 1 Credentials (Linkedin_source)
# ---------------------------
LINKEDIN_DB_HOST = os.getenv("LINKEDIN_DB_HOST")
LINKEDIN_DB_USER = os.getenv("LINKEDIN_DB_USER")
LINKEDIN_DB_PASSWORD = os.getenv("LINKEDIN_DB_PASSWORD")
LINKEDIN_DB_NAME = os.getenv("LINKEDIN_DB_NAME")


# ---------------------------
# Database 2 Credentials (Naukri_source)
# ---------------------------
NAUKRI_DB_HOST = os.getenv("NAUKRI_DB_HOST")
NAUKRI_DB_USER = os.getenv("NAUKRI_DB_USER")
NAUKRI_DB_PASSWORD = os.getenv("NAUKRI_DB_PASSWORD")
NAUKRI_DB_NAME = os.getenv("NAUKRI_DB_NAME")


# ---------------------------
# Global Schema: UnifiedJobPosting
# ---------------------------
GLOBAL_SCHEMA = {
    "job_id": "TEXT",
    "title": "TEXT",
    "company_name": "TEXT",
    "description": "TEXT",
    "skills": "TEXT",
    "experience_required": "TEXT",
    "qualifications": "TEXT",
    "location": "TEXT",
    "country": "TEXT",
    "work_type": "TEXT",
    "salary_range": "TEXT",
    "currency": "TEXT",
    "job_posting_date": "DATETIME",
    "role_category": "TEXT"
}

# ---------------------------
# GAV Mappings (Global-As-View)
# Map each global attribute to source-specific columns
# ---------------------------
GAV_MAPPINGS = {
    "Linkedin_source": {
        "job_id": "job_id",
        "title": "title",
        "company_name": "company_name",
        "description": "description",
        "skills": "skills_desc",
        "experience_required": "formatted_experience_level",
        "qualifications": None,  # Not available in source 1
        "location": "location",
        "country": None,  # Not available in source 1
        "work_type": "formatted_work_type",
        "salary_range": "normalized_salary",
        "currency": "currency",
        "job_posting_date": "listed_time",
        "role_category": None
    },
    "Naukri_source": {
        "job_id": "Job Id",
        "title": "Job Title",
        "company_name": "Company",
        "description": "Job Description",
        "skills": "skills",
        "experience_required": "Experience",
        "qualifications": "Qualifications",
        "location": "location",
        "country": "Country",
        "work_type": "Work Type",
        "salary_range": "Salary Range",
        "currency": None,
        "job_posting_date": "Job Posting Date",
        "role_category": "Role"
    }
}

# ---------------------------
# Map source to actual DB table name
# ---------------------------
SOURCE_TO_TABLE = {
    "Linkedin_source": "jobs",               
    "Naukri_source": "job_listings"        
}

# ---------------------------
# Map source to actual DB platform
# ---------------------------
SOURCE_TO_DB_Type = {
    "GLOBAL_SCHEMA": "MySQL", 
    "Linkedin_source": "MySQL",               
    "Naukri_source": "MySQL"     
}

// D:\Projects\NextMove\constants\__init__.py
import os

# =========================================
# 1. FILE PATHS & SYSTEM CONFIG
# =========================================

QUERY_INPUT_FILE_PATH = r"D:\Projects\NextMove\workspace_folder\input\natural_queries.txt"
QUERY_ANALYZE_OUTPUT_FILE_PATH = r"D:\Projects\NextMove\workspace_folder\artifacts\query_analysis.jsonl"
QUERY_DECOMPOSE_OUTPUT_FILE_PATH = r"D:\Projects\NextMove\workspace_folder\artifacts\query_decompose.jsonl"

# --- HISTORY CONFIGURATION (Use Directory, not single file) ---
HISTORY_DIR_PATH = r"D:\Projects\NextMove\workspace_folder\artifacts\history"
HISTORY_LIMIT_K = 3  # Summarize context after every 3 turns

DEFAULT_LIMIT = 10

# =========================================
# 2. GLOBAL SCHEMA
# =========================================
GLOBAL_SCHEMA = {
    "title": "Job title (e.g. Software Engineer, Marketing Manager)",
    "company_name": "Name of the hiring company",
    "location": "City, State, Country, or 'Remote'",
    "skills": "List of required technical or soft skills (e.g. Python, SQL)",
    "salary_range": "Numeric salary or text range",
    "work_type": "Employment type (e.g. Full-time, Contract, Remote)"
}

# =========================================
# 3. LLM CONFIGURATION
# =========================================
LLM_GEMINI = "gemini"
LLM_GROQ = "groq"

CURRENT_LLM = LLM_GEMINI 
# =========================================

# =========================================
# 4. GEMINI SPECIFIC PROMPTS
# =========================================

GEMINI_QUERY_ANALYZER_SYSTEM_PROMPT = """
You are the Query Analyzer for the NextMove job search system.

INPUT:
- Context: Previous conversation summary
- Query: Latest user message
- Semantic Hints: Synonyms or related skills derived from a Knowledge Graph.

TASKS:

1. RESOLVE INTENT
   - Merge Context + Query to form a standalone ‚Äúuser_intent‚Äù that preserves the user‚Äôs tone, purpose, and specifics. Put more weightage on current query.
   - Replace pronouns (‚Äúthere‚Äù, ‚Äúthat role‚Äù, ‚Äúthis city‚Äù) with explicit entities from Context.

2. CLASSIFY & GENERATE
   - If the intent (fully or partially) can be answered using the database (job listings, filters, salary, companies, locations):
       ‚Üí Generate structured_query and full SQL using the GLOBAL_SCHEMA.
   - If any part of the intent cannot be answered from the database but is still job-related:
       ‚Üí Put that part into unstructured_query only if it is job or job search related otherwise NULL.
   - If both conditions apply ‚Üí generate both.
   - Set `limit` from the user request if stated; otherwise use DEFAULT_LIMIT.
   - Use the Semantic Hints to expand the search.
   - **CRITICAL RULE**: If the Semantic Hints suggest a synonym (e.g., AI -> Artificial Intelligence), or an implicit skill (Data Scientist -> Python), you MUST include them in the SQL using `OR` logic and `LIKE` operators.
   - Example: If user says "AI jobs" and hint says "AI implies Artificial Intelligence", generate:
     `WHERE (title LIKE '%AI%' OR title LIKE '%Artificial Intelligence%')`
   - ALWAYS use `LIKE '%term%'` for text columns (`title`, `company_name`, `location`, `skills`, `description`).
   - NEVER use `=` for text columns unless searching for an exact ID or Code.

3. **OUTPUT JSON:**
   - `user_intent`: The fully resolved, standalone user request.
   - `limit`: Integer (Default: {DEFAULT_LIMIT}).
   - `structured_query`: {{ "select_clause": [...], "where_clause": {{...}} }}
   - `sql_query`: Full Standard SQL string using Global Schema ({schema}). Use `LIMIT`.
   - `unstructured_query`: Text question for the LLM job related only(or null).

Global Schema Attributes: {schema}
"""

GEMINI_RESULT_SYNTHESIZER_SYSTEM_PROMPT = """
You are a helpful assistant for the NextMove job search platform.

Input JSON contains:
- User Intent (main request)
- Unstructured Query (optional general question)
- Database Results (job listings, bypassed, or error)

Rules:

1. If Unstructured Query exists AND Database Results are empty or "Bypassed":
   ‚Üí Answer the question using your own general knowledge.

2. If Database Results contain an error:
   ‚Üí Give a brief, polite apology for the technical issue only if the user wanted job listings otherwise only answer the unstructured query.

3. If job listings are provided and relevant:
   ‚Üí Present them cleanly in this flexible format:

      ‚Ä¢ Job Title
        Company: <company name>
        Location: <location>
        Type: <full-time/part-time/remote> (optional)
        Salary: <salary info> (optional)
        Experience: <experience info> (optional)

      (Only show fields that exist.)

4. Always respond in a clear, helpful, conversational tone.
5. Do NOT output JSON‚Äîonly the final user-facing answer.
6. If there is not no relevant information, respond politely that you couldn't find anything.
7. if user is asking any irrelevant question to job search domain, politely refuse to answer.
"""

GEMINI_RETRY_SYSTEM_PROMPT = """
System: SQL Fixer.
Output: JSON {{ "corrected_sql": "SELECT ..." }}
"""

GEMINI_TRANSLATE_RETRY_SYSTEM_PROMPT = """
System: SQL Translator Fixer.
Output: JSON {{ "corrected_sql": "SELECT ..." }}
"""

GEMINI_SUMMARIZER_PROMPT = """
You are a Conversation Summarizer. 
Your goal is to condense the conversation history into a concise context summary.
Input format: Current Summary + Recent Interaction.
Output: Return ONLY the new updated summary text.
"""

# =========================================
# 5. GROQ SPECIFIC PROMPTS
# =========================================

GROQ_QUERY_ANALYZER_SYSTEM_PROMPT = """
You are the Query Analyzer for the NextMove job search system.

INPUT:
- Context: Previous conversation summary
- Query: Latest user message

TASKS:

1. RESOLVE INTENT
   - Merge Context + Query to form a standalone ‚Äúuser_intent‚Äù that preserves the user‚Äôs tone, purpose, and specifics. Put more weightage on current query.
   - Replace pronouns (‚Äúthere‚Äù, ‚Äúthat role‚Äù, ‚Äúthis city‚Äù) with explicit entities from Context.

2. CLASSIFY & GENERATE
   - If the intent (fully or partially) can be answered using the database (job listings, filters, salary, companies, locations):
       ‚Üí Generate structured_query and full SQL using the GLOBAL_SCHEMA.
   - If any part of the intent cannot be answered from the database but is still job-related:
       ‚Üí Put that part into unstructured_query only if it is job or job search related otherwise NULL.
   - If both conditions apply ‚Üí generate both.
   - Set `limit` from the user request if stated; otherwise use DEFAULT_LIMIT.
   - ALWAYS use `LIKE '%term%'` for text columns (`title`, `company_name`, `location`, `skills`, `description`).
   - NEVER use `=` for text columns unless searching for an exact ID or Code.

3. **OUTPUT JSON:**
   - `user_intent`: The fully resolved, standalone user request.
   - `limit`: Integer (Default: {DEFAULT_LIMIT}).
   - `structured_query`: {{ "select_clause": [...], "where_clause": {{...}} }}
   - `sql_query`: Full Standard SQL string using Global Schema ({schema}). Use `LIMIT`.
   - `unstructured_query`: Text question for the LLM job related only(or null).

Global Schema Attributes: {schema}
"""

GROQ_RESULT_SYNTHESIZER_SYSTEM_PROMPT = """
You are a helpful assistant for the NextMove job search platform.

Input JSON contains:
- User Intent (main request)
- Unstructured Query (optional general question)
- Database Results (job listings, bypassed, or error)

Rules:

1. If Unstructured Query exists AND Database Results are empty or "Bypassed":
   ‚Üí Answer the question using your own general knowledge.

2. If Database Results contain an error:
   ‚Üí if the user wanted job listings Give a short, polite apology for the technical issue, otherwise only answer the unstructured query.

3. If job listings are provided and relevant:
   ‚Üí Present them cleanly in this flexible format:

      ‚Ä¢ Job Title
        Company: <company name>
        Location: <location>
        Type: <full-time/part-time/remote> (optional)
        Salary: <salary info> (optional)
        Experience: <experience info> (optional)

      (Only show fields that exist.)

4. Always respond in a clear, helpful, conversational tone.
5. Do NOT output JSON‚Äîonly the final user-facing answer.
6. If there is not no relevant information, respond politely that you couldn't find anything.
7. if user is asking any irrelevant question to job search domain, politely refuse to answer.
"""

GROQ_RETRY_SYSTEM_PROMPT = """
System: SQL Correction Mode.
Task: Fix the invalid SQL query based on the error message.
Output: JSON ONLY. Format: {{ "corrected_sql": "SELECT ..." }}
"""

GROQ_TRANSLATE_RETRY_SYSTEM_PROMPT = """
System: SQL Translation Correction.
Task: Adapt the Global SQL to the Local Schema based on the validation error.
Output: JSON ONLY. Format: {{ "corrected_sql": "SELECT ..." }}
"""

GROQ_SUMMARIZER_PROMPT = """
You are a Conversation Summarizer. 
Your goal is to condense the conversation history into a concise context summary.
Input format: Current Summary + Recent Interaction.
Output: Return ONLY the new updated summary text.
"""

# =========================================
# 6. PROMPT REGISTRY
# =========================================

# =========================================
# 6. PROMPT REGISTRY
# =========================================

PROMPT_REGISTRY = {
    LLM_GEMINI: {
        "analyzer_system": GEMINI_QUERY_ANALYZER_SYSTEM_PROMPT,
        "synthesizer_system": GEMINI_RESULT_SYNTHESIZER_SYSTEM_PROMPT,
        "retry_global": GEMINI_RETRY_SYSTEM_PROMPT,
        "retry_translation": GEMINI_TRANSLATE_RETRY_SYSTEM_PROMPT,
        "summarizer": GEMINI_SUMMARIZER_PROMPT 
    },
    LLM_GROQ: {
        "analyzer_system": GROQ_QUERY_ANALYZER_SYSTEM_PROMPT,
        "synthesizer_system": GROQ_RESULT_SYNTHESIZER_SYSTEM_PROMPT,
        "retry_global": GROQ_RETRY_SYSTEM_PROMPT,
        "retry_translation": GROQ_TRANSLATE_RETRY_SYSTEM_PROMPT,
        "summarizer": GROQ_SUMMARIZER_PROMPT 
    }
}

def get_current_prompts():
    if CURRENT_LLM not in PROMPT_REGISTRY:
        # Fallback for safety
        return PROMPT_REGISTRY[LLM_GEMINI]
    return PROMPT_REGISTRY[CURRENT_LLM]

CURRENT_PROMPTS = get_current_prompts()

# Shared Human Prompts
QUERY_ANALYZER_HUMAN_PROMPT = """Input: {user_query}\nOutput:"""
RESULT_SYNTHESIZER_HUMAN_PROMPT = """Resolved Intent: {user_intent}\nUnstructured Query: {unstructured_query}\nDatabase Results:\n{database_results_json}\nAnswer:"""

// D:\Projects\NextMove\components\validator\SQLValidator.py
import sqlglot
from sqlglot import parse_one, transpile, exp
from sqlglot.errors import ParseError, TokenError, UnsupportedError
from typing import Dict, List, Optional

class SQLValidator:
    DIALECTS = ['postgres', 'mysql']

    def __init__(self, dialect: str = 'postgres', schema: Optional[Dict[str, List[str]]] = None):
        self.dialect = dialect if dialect in self.DIALECTS else 'postgres'
        self.schema = schema or {}

    def validate(self, query: str) -> Dict:
        result = {
            'is_valid': False,
            'message': '',
            'errors': [],
            'warnings': [],
            'query_type': None,
            'parsed_tree': None
        }

        if not query or not query.strip():
            result['errors'].append('Query is empty')
            result['message'] = 'Invalid: Query is empty'
            return result

        try:
            parsed = parse_one(query, read=self.dialect, error_level=None)

            # Early syntax checks before semantic validation
            syntax_errors = self._check_common_syntax(parsed)
            if syntax_errors:
                result['errors'].extend(syntax_errors)
                result['message'] = 'Invalid: Syntax errors'
                return result

            # Check for empty INSERT VALUES
            if isinstance(parsed, sqlglot.exp.Insert):
                if not parsed.expressions or all(e is None for e in parsed.expressions):
                    result['errors'].append("INSERT has empty VALUES list")
                    result['message'] = "Invalid: Empty VALUES in INSERT"
                    return result

            # If we get here, the query is syntactically valid
            result['is_valid'] = True
            result['message'] = 'Query syntax is valid'
            result['query_type'] = type(parsed).__name__
            result['parsed_tree'] = parsed.sql(pretty=True)

            # Warnings for potentially dangerous operations
            result['warnings'] = self._check_warnings(parsed)

            # Semantic validation if schema is provided
            if self.schema:
                sem_errors = self._check_semantics(parsed)
                result['errors'].extend(sem_errors)
                if sem_errors:
                    result['is_valid'] = False
                    result['message'] = 'Invalid: Semantic errors'

        except ParseError as e:
            result['errors'].append(f'Parse error: {str(e)}')
            result['message'] = 'Invalid: Parse error'
        except TokenError as e:
            result['errors'].append(f'Token error: {str(e)}')
            result['message'] = 'Invalid: Token error'
        except UnsupportedError as e:
            result['errors'].append(f'Unsupported syntax: {str(e)}')
            result['message'] = 'Invalid: Unsupported syntax'
        except Exception as e:
            result['errors'].append(f'Unexpected error: {str(e)}')
            result['message'] = 'Invalid: Unexpected error'

        return result

    def _check_common_syntax(self, parsed) -> List[str]:
        """Check for missing FROM in SELECT or target table in UPDATE/DELETE."""
        errors = []

        # SELECT must have FROM
        for select in parsed.find_all(exp.Select):
            if not select.args.get("from"):
                errors.append('SELECT statement missing FROM clause')

        # UPDATE must have target table
        for update in parsed.find_all(exp.Update):
            if not update.this:
                errors.append('UPDATE statement missing target table')

        # DELETE must have FROM clause
        for delete in parsed.find_all(exp.Delete):
            if not delete.args.get('this'):
                errors.append('DELETE statement missing FROM clause')

        return errors

    def _check_warnings(self, parsed) -> List[str]:
        warnings = []

        # UPDATE without WHERE
        for update in parsed.find_all(exp.Update):
            if not update.args.get('where'):
                warnings.append('UPDATE without WHERE clause will modify all rows')

        # DELETE without WHERE
        for delete in parsed.find_all(exp.Delete):
            if not delete.args.get('where'):
                warnings.append('DELETE without WHERE clause will remove all rows')

        # SELECT *
        for select in parsed.find_all(exp.Select):
            if any(select.find_all(exp.Star)):
                warnings.append('SELECT * may have performance implications in production')

        # Empty INSERT VALUES
        for insert in parsed.find_all(exp.Insert):
            values = insert.args.get('expressions')
            if not values or all(not v.args for v in values):
                warnings.append('INSERT has empty VALUES list')

        # Missing semicolon (style)
        if not parsed.sql().rstrip().endswith(';'):
            warnings.append('Query does not end with semicolon (optional but recommended)')

        return list(set(warnings))

    def _check_semantics(self, parsed) -> List[str]:
        errors = []

        # Tables in query
        tables_in_query = [t.name for t in parsed.find_all(exp.Table)]
        for table in tables_in_query:
            if table not in self.schema:
                errors.append(f'Table "{table}" does not exist in schema')

        # Columns in query
        for column in parsed.find_all(exp.Column):
            table_name = column.find(exp.Table)
            col_table = table_name.name if table_name else None
            col_name = column.name
            if col_table:
                if col_table in self.schema and col_name not in self.schema[col_table]:
                    errors.append(f'Column "{col_name}" does not exist in table "{col_table}"')
            else:
                if not any(col_name in self.schema[t] for t in tables_in_query if t in self.schema):
                    errors.append(f'Column "{col_name}" does not exist in any referenced table')

        return errors

    def validate_multiple(self, queries: List[str]) -> List[Dict]:
        return [self.validate(q) for q in queries]

    def transpile(self, query: str, target_dialect: str) -> Dict:
        result = self.validate(query)
        if result['is_valid']:
            try:
                transpiled = transpile(query, read=self.dialect, write=target_dialect)[0]
                result['transpiled'] = transpiled
                result['target_dialect'] = target_dialect
            except Exception as e:
                result['errors'].append(f'Transpilation error: {str(e)}')
        return result

    def format_query(self, query: str, pretty: bool = True) -> Dict:
        result = self.validate(query)
        if result['is_valid']:
            try:
                parsed = parse_one(query, read=self.dialect)
                result['formatted'] = parsed.sql(pretty=pretty, dialect=self.dialect)
            except Exception as e:
                result['errors'].append(f'Formatting error: {str(e)}')
        return result




// D:\Projects\NextMove\components\validator\SQLValidatorWrapper.py
from typing import Dict, Optional
from components.validator.SQLValidator import SQLValidator
from entities.config import GLOBAL_SCHEMA, GAV_MAPPINGS, SOURCE_TO_TABLE, SOURCE_TO_DB_Type


class FederatedSQLValidator:
    """
    Wrapper around SQLValidator to support multiple sources (MySQL/Postgres)
    and semantic validation using GAV mappings and global schema.
    """

    def __init__(self):
        # Create validator instances for MySQL and Postgres
        self.validators = {
            "MySQL": SQLValidator(dialect="mysql"),
            "PostgreSQL": SQLValidator(dialect="postgres")
        }

    def get_source_schema(self, source_name: str) -> Dict[str, list]:
        """
        Generate a table->columns schema dictionary for SQLValidator from GAV mappings.
        """
        schema = {}
        if source_name in GAV_MAPPINGS and source_name in SOURCE_TO_TABLE:
            table_name = SOURCE_TO_TABLE[source_name]
            columns_mapping = GAV_MAPPINGS[source_name]
            # Only include columns that exist in source
            columns = [col for col in columns_mapping.values() if col]
            schema[table_name] = columns
        return schema

    def validate_query(self, query: str, source_name: Optional[str] = None) -> Dict:
        """
        Validate a query for a specific source.
        If source_name is None, uses GLOBAL_SCHEMA.
        """
        # Determine SQL dialect
        dialect = "mysql"  # default
        if source_name and source_name in SOURCE_TO_DB_Type:
            db_type = SOURCE_TO_DB_Type[source_name]
            dialect = "mysql" if db_type.lower() == "mysql" else "postgres"

        # Pick validator instance
        validator = self.validators["MySQL"] if dialect == "mysql" else self.validators["PostgreSQL"]

        # Determine schema
        schema = self.get_source_schema(source_name) if source_name else { "global": list(GLOBAL_SCHEMA.keys()) }

        # Update validator schema
        validator.schema = schema

        # Validate query
        result = validator.validate(query)
        return result



// D:\Projects\NextMove\components\translator\MySQL_translator.py
import re
from sqlglot import parse_one, exp
from entities.config import GAV_MAPPINGS

# Regex for safe unquoted identifier (lowercase, numbers, underscore)
IDENT_RE = re.compile(r"^[a-z_][a-z0-9_]*$")

class SQLTranslator:
    def __init__(self, source: str, global_table_names=None, target_table_name: str = None, dialect: str = "mysql"):
        """
        source: name of the target source (e.g., 'Linkedin_source')
        target_table_name: The actual physical table name (e.g., 'jobs')
        """
        if source not in GAV_MAPPINGS:
            raise ValueError(f"Source '{source}' is not defined in GAV_MAPPINGS.")
        if dialect.lower() not in ("mysql", "postgres"):
            raise ValueError("Dialect must be either 'mysql' or 'postgres'")

        self.source = source
        self.dialect = dialect.lower()
        self.target_table_name = target_table_name

        # --- FIX: Add 'job_listings' and 'job_postings' to the detection list ---
        # This ensures that if the LLM generates 'FROM job_listings', we catch it and map it.
        self.global_table_names = global_table_names or [
            "jobs", 
            "job_listings", 
            "job_postings", 
            "Global_Job_Postings",
            "unified_job_posting"
        ]
        
        self.column_mapping = self._generate_column_mapping(source)
        self.table_mapping = self._generate_table_mapping()

    def _generate_column_mapping(self, source):
        mapping = {}
        gav = GAV_MAPPINGS.get(source, {})
        for global_col, source_col in gav.items():
            if source_col:
                # Normalize to lowercase for consistent lookup
                mapping[global_col.lower()] = source_col
        return mapping

    def _generate_table_mapping(self):
        """
        Maps ALL detected global table variations to the single target table.
        """
        table_map = {}
        if self.target_table_name:
            for g in self.global_table_names:
                # Map both raw and lowercase versions to be safe
                table_map[g] = self.target_table_name
                table_map[g.lower()] = self.target_table_name
        return table_map

    def _should_quote(self, identifier: str) -> bool:
        if identifier is None:
            return False
        if self.dialect == "postgres":
            return not bool(IDENT_RE.match(identifier))
        if self.dialect == "mysql":
            # Quote if it contains spaces or special chars
            return not bool(re.match(r"^[A-Za-z0-9_]+$", identifier))
        return False

    def _quote_if_needed(self, identifier: str) -> str:
        if identifier is None:
            return identifier
        if self._should_quote(identifier):
            if self.dialect == "postgres":
                safe = identifier.replace('"', '""')
                return f'"{safe}"'
            elif self.dialect == "mysql":
                return f"`{identifier}`"
        return identifier

    def _map_table(self, table_name: str) -> str:
        if not table_name:
            return table_name
        
        # Case-insensitive lookup
        lower_name = table_name.lower()
        
        # Check if this table name is in our "Global List" that needs mapping
        # OR if we simply have a direct mapping for it
        mapped_name = self.table_mapping.get(lower_name, self.table_mapping.get(table_name))
        
        if mapped_name:
            return self._quote_if_needed(mapped_name)
        
        # If not found in mapping, return original (quoted if needed)
        return self._quote_if_needed(table_name)

    def _map_column_lookup(self, table, col):
        # 1. Try fully qualified lookup (table.col)
        if table:
            # We don't rely on table name here for mapping keys, usually just global column name
            pass
        
        # 2. Try simple column lookup (case-insensitive input)
        return self.column_mapping.get(col.lower(), col)

    def _extract_table_name(self, node):
        if node is None: return None
        if isinstance(node, str): return node
        if isinstance(node, exp.Table):
            return node.name
        return str(node)

    def _assign_column_node(self, col_node: exp.Column, parent_table):
        if not isinstance(col_node, exp.Column):
            return
        
        # Get mapped column name
        mapped_col_name = self._map_column_lookup(None, col_node.name)

        # If specific source requires table prefix (e.g. "t1.col"), split it
        if isinstance(mapped_col_name, str) and "." in mapped_col_name:
            table_part, col_part = mapped_col_name.split(".", 1)
            col_node.set("this", exp.Identifier(this=col_part, quoted=self._should_quote(col_part)))
            col_node.set("table", exp.Identifier(this=table_part, quoted=self._should_quote(table_part)))
        else:
            # Update the column name
            col_node.set("this", exp.Identifier(this=mapped_col_name, quoted=self._should_quote(mapped_col_name)))
            
            # IMPORTANT: If the column had a table alias/name attached (e.g. job_listings.title),
            # we must update that table identifier to the LOCAL table name too.
            if col_node.table:
                mapped_table = self._map_table(col_node.table)
                # Remove quotes for the Identifier constructor, it adds them based on quoted=True
                clean_table = mapped_table.replace("`", "").replace('"', "")
                col_node.set("table", exp.Identifier(this=clean_table, quoted=True))

    def _replace_recursive(self, node, parent_table=None, visited=None):
        if visited is None:
            visited = set()
        if not isinstance(node, exp.Expression):
            return
        
        node_id = id(node)
        if node_id in visited:
            return
        visited.add(node_id)

        # 1. Handle Table Nodes (FROM clauses, JOINs)
        if isinstance(node, exp.Table):
            tname = self._extract_table_name(node)
            if tname:
                mapped = self._map_table(tname)
                # Update node
                node.set("this", exp.Identifier(this=mapped.replace("`", "").replace('"', ""), quoted=True))
            if node.alias:
                parent_table = node.alias

        # 2. Handle Column Nodes (SELECT list, WHERE clauses)
        if isinstance(node, exp.Column):
            self._assign_column_node(node, parent_table)

        # 3. Recurse into args/expressions
        for arg in node.args.values():
            if isinstance(arg, list):
                for item in arg:
                    if isinstance(item, exp.Expression):
                        self._replace_recursive(item, parent_table, visited)
            elif isinstance(arg, exp.Expression):
                self._replace_recursive(arg, parent_table, visited)

    def translate_query(self, query: str) -> str:
        """
        Translate a query from global schema to target source SQL using sqlglot.
        """
        try:
            # Parse
            tree = parse_one(query)
            
            # Transform
            self._replace_recursive(tree)
            
            # Generate SQL
            return tree.sql(dialect=self.dialect)
            
        except Exception as e:
            print(f"[Translator Error] {e}")
            return query

// D:\Projects\NextMove\components\synthesizer\result_synthesizer.py
import json
from typing import Dict, Any
from constants import (
    CURRENT_PROMPTS, 
    RESULT_SYNTHESIZER_HUMAN_PROMPT,
    CURRENT_LLM
)
from components.LLM.llm_loader import load_llm
from langchain_core.messages import SystemMessage, HumanMessage


def synthesize_results(
    natural_language_query: str,
    unstructured_query: str,
    database_results: Dict[str, Any],
    user_intent: str = None # <--- NEW: Context Resolved Intent
) -> Dict[str, Any]:
    """
    Takes retrieved data, calls LLM, and returns a dictionary with
    the final answer and the prompts used.
    """
    print("[STEP 4] Synthesizing final answer with LLM...")

    try:
        results_json = json.dumps(database_results, indent=2, default=str)
    except Exception as e:
        print(f"[WARN] Could not serialize DB results to JSON: {e}")
        results_json = str(database_results)

    # --- Format prompts ---
    
    # 1. Retrieve the correct System Prompt dynamically based on CURRENT_LLM
    system_prompt_str = CURRENT_PROMPTS["synthesizer_system"]
    
    # Use resolved intent if available, else raw query
    intent_to_use = user_intent if user_intent else natural_language_query

    # 2. Format human prompt
    human_prompt_str = RESULT_SYNTHESIZER_HUMAN_PROMPT.format(
        user_intent=intent_to_use, # <--- Use Resolved Intent
        unstructured_query=unstructured_query or "None",
        database_results_json=results_json
    )
    # ---

    try:
        # 2. Load the LLM using your loader
        llm = load_llm(CURRENT_LLM, temperature=0.1)

        # 3. Create LangChain messages
        messages = [
            SystemMessage(content=system_prompt_str),
            HumanMessage(content=human_prompt_str)
        ]
        
        print(f"\n[LLM Call] Invoking {CURRENT_LLM} for synthesis...")
        
        # 4. Invoke the model
        response = llm.invoke(messages)
        
        # 5. Get the text content
        final_answer = response.content
        
        # 6. Return a dictionary
        return {
            "final_answer": final_answer,
            "prompts_used": {
                "system": system_prompt_str,
                "human": human_prompt_str
            }
        }
        
    except Exception as e:
        print(f"[ERROR] Final LLM synthesis failed: {e}")
        # Return a dictionary even on error
        return {
            "final_answer": "I was able to retrieve the data, but I encountered an error when trying to formulate a final answer. Here is the raw data: " + results_json,
            "prompts_used": {
                "system": system_prompt_str,
                "human": human_prompt_str,
                "error": str(e)
            }
        }

// D:\Projects\NextMove\components\LLM\llm_loader.py
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_groq import ChatGroq 

def load_llm(llm_name: str, temperature: float = 0.0):
    """
    Load and return a LangChain LLM instance (Gemini or Ollama/Groq)
    based on the provided llm_name.
    """

    # Load .env from project root
    root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
    env_path = os.path.join(root_dir, ".env")
    load_dotenv(env_path)

    llm_name = llm_name.lower()

    if "gemini" in llm_name:
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in .env")

        return ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=temperature,
            max_retries=2,
            google_api_key=api_key,
        )

    elif "groq" in llm_name:
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            raise ValueError("GROQ_API_KEY not found in .env")
            
        # UPDATED: Switched to Llama 3.3 70B (Versatile) as the previous model is decommissioned
        return ChatGroq(
            model="llama-3.3-70b-versatile", 
            temperature=temperature,
            max_retries=2,
            api_key=api_key
        )

    else:
        raise ValueError(f"Unsupported LLM name: {llm_name}")

// D:\Projects\NextMove\components\LLM\query_retry_handler.py
import json
from langchain_core.prompts import ChatPromptTemplate
from ..LLM.llm_loader import load_llm
from constants import CURRENT_LLM, CURRENT_PROMPTS, GLOBAL_SCHEMA

# Load current LLM dynamically
llm = load_llm(CURRENT_LLM)

def parse_llm_json_response(llm_response: str) -> dict:
    """
    Parse LLM JSON response wrapped in triple backticks if present.
    """
    raw_content = llm_response.content.strip() if hasattr(llm_response, 'content') else str(llm_response).strip()

    # Remove surrounding triple backticks
    if raw_content.startswith("```") and raw_content.endswith("```"):
        lines = raw_content.splitlines()
        if lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].startswith("```"):
            lines = lines[:-1]
        raw_content = "\n".join(lines).strip()

    try:
        result_json = json.loads(raw_content)
    except json.JSONDecodeError as e:
        # Simple fallback to try and find brace boundaries
        try:
            start = raw_content.find('{')
            end = raw_content.rfind('}') + 1
            result_json = json.loads(raw_content[start:end])
        except:
            raise ValueError(f"Failed to parse JSON from LLM response: {e}")

    return result_json


class QueryRetryHandler:
    """
    Handles retries for invalid SQL queries:
    1. Global SQL retry
    2. Source-specific translation retry
    """

    def __init__(self, max_retries: int = 3):
        self.max_retries = max_retries

    def retry_global_sql(self, natural_query: str, previous_sql: str, validation_errors: list) -> str:
        system_prompt = CURRENT_PROMPTS["retry_global"]

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{user_query}")
        ])

        for _ in range(self.max_retries):
            prompt = prompt_template.invoke({
                "user_query": natural_query,
                "natural_query": natural_query,
                "global_schema": ", ".join(GLOBAL_SCHEMA.keys()),
                "previous_sql": previous_sql,
                "validation_errors": "\n".join(validation_errors)
            })

            # FIX: Use .invoke()
            response = llm.invoke(prompt.messages)
            
            try:
                result = parse_llm_json_response(response)
                corrected_sql = result.get("corrected_sql")
                if corrected_sql:
                    return corrected_sql
            except Exception:
                continue # Try again if parsing fails

        raise RuntimeError(f"Failed to generate valid global SQL after {self.max_retries} retries.")

    def retry_translation(
        self,
        global_sql: str,
        source_name: str,
        db_type: str,
        previous_translation: str,
        local_schema: dict,
        validation_errors: list
    ) -> str:
        system_prompt = CURRENT_PROMPTS["retry_translation"]
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{user_query}")
        ])

        for _ in range(self.max_retries):
            prompt = prompt_template.invoke({
                "user_query": global_sql,
                "global_sql": global_sql,
                "source_name": source_name,
                "db_type": db_type,
                "previous_translation": previous_translation,
                "global_schema": ", ".join(GLOBAL_SCHEMA.keys()),
                "local_schema": json.dumps(local_schema, indent=2),
                "validation_errors": "\n".join(validation_errors)
            })

            # FIX: Use .invoke()
            response = llm.invoke(prompt.messages)
            
            try:
                result = parse_llm_json_response(response)
                corrected_sql = result.get("corrected_sql")
                if corrected_sql:
                    return corrected_sql
            except Exception:
                continue

        raise RuntimeError(f"Failed to generate valid translation for source '{source_name}' after {self.max_retries} retries.")

// D:\Projects\NextMove\components\history_manager\history_handler.py
import json
import os
from langchain_core.messages import SystemMessage, HumanMessage
from components.LLM.llm_loader import load_llm
from constants import (
    HISTORY_DIR_PATH, 
    HISTORY_LIMIT_K, 
    CURRENT_LLM, 
    CURRENT_PROMPTS
)

# --- SAFETY LIMITS ---
MAX_STORED_RESPONSE_LEN = 1500  # Max chars to save per AI response (prevents bloat)
MAX_CONTEXT_CHARS = 6000        # Max characters allowed before forcing a summary (approx 1.5k tokens)

class HistoryHandler:
    def __init__(self, session_id: str = "default"):
        self.session_id = session_id
        self.file_path = os.path.join(HISTORY_DIR_PATH, f"chat_history_{session_id}.json")
        self.limit = HISTORY_LIMIT_K
        self.llm = load_llm(CURRENT_LLM, temperature=0.0)
        self._load_history()

    def _load_history(self):
        """Loads history from JSON or initializes empty structure."""
        if os.path.exists(self.file_path):
            try:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.summary = data.get("summary", "")
                    self.recent_turns = data.get("recent_turns", [])
            except json.JSONDecodeError:
                self._init_empty_history()
        else:
            self._init_empty_history()

    def _init_empty_history(self):
        self.summary = ""
        self.recent_turns = []
        self._save_history()

    def _save_history(self):
        """Persists current state to disk."""
        os.makedirs(os.path.dirname(self.file_path), exist_ok=True)
        data = {
            "summary": self.summary,
            "recent_turns": self.recent_turns
        }
        with open(self.file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)

    def get_context_string(self) -> str:
        """
        Returns the formatted context.
        **LOGIC CHECK (ii):** If context > MAX_CONTEXT_CHARS, force summary immediately.
        """
        # 1. Construct the raw context string
        context_str = ""
        if self.summary:
            context_str += f"PREVIOUS SUMMARY: {self.summary}\n\n"
        
        if self.recent_turns:
            turns_text = "\n".join([f"User: {t['user']}\nAI: {t['ai']}\n" for t in self.recent_turns])
            context_str += f"RECENT INTERACTION LOG:\n{turns_text}"

        # 2. CHECK LENGTH
        if len(context_str) > MAX_CONTEXT_CHARS:
            print(f"[History] Context length ({len(context_str)}) exceeds limit ({MAX_CONTEXT_CHARS}). Forcing summarization...")
            self._summarize_and_prune() # <--- Consolidates history
            
            # Re-construct the string (It will now contain only the new Summary)
            context_str = f"PREVIOUS SUMMARY: {self.summary}\n\n"
        
        return context_str

    def add_interaction(self, user_query: str, ai_response: str):
        """
        Adds a turn and checks if K limit is reached.
        """
        # Safety: Truncate massive responses before storing
        clean_response = ai_response
        if len(ai_response) > MAX_STORED_RESPONSE_LEN:
            keep = MAX_STORED_RESPONSE_LEN // 2
            clean_response = ai_response[:keep] + "\n... [TRUNCATED] ...\n" + ai_response[-keep:]

        self.recent_turns.append({
            "user": user_query,
            "ai": clean_response
        })
        
        # **LOGIC CHECK (i):** Check Turn Count Limit
        if len(self.recent_turns) >= self.limit:
            print(f"[History] Turn limit {self.limit} reached. Summarizing...")
            self._summarize_and_prune()
        
        self._save_history()

    def _summarize_and_prune(self):
        """
        Uses LLM to merge (Summary + Recent Turns) -> (New Summary).
        Clears Recent Turns.
        """
        system_prompt = CURRENT_PROMPTS["summarizer"]
        
        recent_text = "\n".join([f"User: {t['user']}\nAI: {t['ai']}" for t in self.recent_turns])
        
        human_input = (
            f"Current Summary: {self.summary if self.summary else 'None'}\n\n"
            f"Recent Interactions to merge:\n{recent_text}"
        )

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_input)
        ]

        try:
            response = self.llm.invoke(messages)
            new_summary = response.content.strip()
            
            # Update State
            self.summary = new_summary
            self.recent_turns = [] # Clear buffer
            
            print(f"[History] Summary updated successfully.")
            self._save_history() # Save immediately
            
        except Exception as e:
            print(f"[Error] Failed to summarize history: {e}")

// D:\Projects\NextMove\components\connectors\mysql_connector.py
# D:\Projects\NextMove\components\connectors\mysql_connector.py

import mysql.connector
from mysql.connector import Error
from typing import List, Dict, Any

class MySQLConnector:
    """
    A MySQL connector class that supports connecting to
    specific databases with provided credentials and a timeout.
    """
    def __init__(self, host, user, password, database, timeout=5):
        self.host = host
        self.user = user
        self.password = password
        self.database = database
        self.port = 3306
        self.timeout = timeout  # Connection timeout
        self.connection = None
        self.cursor = None

    def connect(self):
        """Establishes the database connection with a timeout."""
        try:
            self.connection = mysql.connector.connect(
                host=self.host,
                user=self.user,
                password=self.password,
                database=self.database,
                port=self.port,
                connection_timeout=self.timeout # Fail fast if DB is down
            )
            if self.connection.is_connected():
                self.cursor = self.connection.cursor()
        except Error as e:
            print(f"[ERROR] Could not connect to {self.database}: {e}")
            raise e

    def disconnect(self):
        """Closes the database connection."""
        if self.connection and self.connection.is_connected():
            self.cursor.close()
            self.connection.close()

    def execute_query_as_dict(self, query: str) -> List[Dict[str, Any]]:
        """
        Executes a query and returns the results as a list of dictionaries.
        This is essential for JSON serialization and for the LLM.
        """
        if not self.connection or not self.cursor:
            print("[ERROR] Not connected. Call connect() first.")
            return []
            
        try:
            self.cursor.execute(query)
            rows = self.cursor.fetchall()
            # Get column names from cursor description
            columns = [col[0] for col in self.cursor.description]
            
            # Convert list of tuples to list of dicts
            result_list = [dict(zip(columns, row)) for row in rows]
            return result_list
            
        except Error as e:
            print(f"[ERROR] Query failed: {e}\nQuery: {query}")
            # Raise the exception so the pipeline can catch it
            raise e

    def execute_query(self, query: str):
        """Executes a query and returns raw tuples."""
        if not self.connection or not self.cursor:
            print("[ERROR] Not connected. Call connect() first.")
            return []
        
        try:
            self.cursor.execute(query)
            rows = self.cursor.fetchall()
            return rows
        except Error as e:
            print(f"[ERROR] Query failed: {e}\nQuery: {query}")
            raise e

// D:\Projects\NextMove\components\analyzer_and_decomposer\query_analyzer.py
# components/analyzer_and_decomposer/query_analyzer.py
from langchain_core.prompts import ChatPromptTemplate
from ..LLM.llm_loader import load_llm
from constants import CURRENT_LLM, CURRENT_PROMPTS, DEFAULT_LIMIT, GLOBAL_SCHEMA
# --- NEW IMPORT ---
from components.matcher.term_normalizer import TermNormalizer 
import json

# Load current LLM
llm = load_llm(CURRENT_LLM)
# Initialize Normalizer
normalizer = TermNormalizer()

def parse_llm_json_response(llm_response: str) -> dict:
    # ... (Keep existing logic unchanged) ...
    raw_content = llm_response.content.strip() if hasattr(llm_response, 'content') else str(llm_response).strip()
    if raw_content.startswith("```") and raw_content.endswith("```"):
        lines = raw_content.splitlines()
        if lines[0].startswith("```"): lines = lines[1:]
        if lines[-1].startswith("```"): lines = lines[:-1]
        raw_content = "\n".join(lines).strip()
    try:
        result_json = json.loads(raw_content)
    except json.JSONDecodeError:
        try:
            start = raw_content.find('{')
            end = raw_content.rfind('}') + 1
            if start != -1 and end != -1:
                result_json = json.loads(raw_content[start:end])
            else: raise ValueError("No JSON found")
        except:
            raise ValueError(f"Failed to parse JSON: {raw_content}")
    return result_json

def query_analyze(natural_query: str, chat_history_context: str = ""):
    """
    Analyzes the query with semantic expansion.
    """
    # Get system prompt
    system_prompt = CURRENT_PROMPTS["analyzer_system"]
    
    # --- SEMANTIC EXPANSION ---
    # Get hints from the graph (Offline Brain)
    semantic_hints = normalizer.expand_query(natural_query)
    
    # --- CONTEXT INJECTION ---
    # Combine History + Semantic Hints + User Query
    full_human_input = (
        f"=== PREVIOUS CONVERSATION CONTEXT ===\n{chat_history_context}\n" if chat_history_context else ""
    )
    
    full_human_input += (
        f"{semantic_hints}\n"
        f"=====================================\n"
        f"LATEST USER QUERY: {natural_query}\n"
    )

    # Debug print to see what LLM receives
    print(f"[QueryAnalyzer] Context Injected:\n{semantic_hints}")

    # Create prompt
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input_content}"),
    ])

    # Fill variables
    prompt = chat_prompt.invoke({
        "DEFAULT_LIMIT": DEFAULT_LIMIT,
        "schema": ", ".join(GLOBAL_SCHEMA.keys()),
        "input_content": full_human_input 
    })

    # Invoke LLM
    response = llm.invoke(prompt.messages)
    result_json = parse_llm_json_response(response)
    
    if result_json and "user_intent" not in result_json:
        result_json["user_intent"] = natural_query

    return result_json

// D:\Projects\NextMove\components\analyzer_and_decomposer\query_decomposer.py
import pprint
from entities.config import GAV_MAPPINGS, SOURCE_TO_TABLE, SOURCE_TO_DB_Type, GLOBAL_SCHEMA
from components.validator.SQLValidatorWrapper import FederatedSQLValidator
from components.translator.MySQL_translator import SQLTranslator
from components.LLM.query_retry_handler import QueryRetryHandler

def prepare_federated_queries(
    analyzed_result: dict,
    max_retries: int = 3,
    use_llm_retry: bool = True
) -> dict:
    """
    Take analyzer result ‚Üí validate global SQL ‚Üí retry if invalid ‚Üí translate to sources ‚Üí retry if invalid.
    Returns structured queries, global_sql, and unstructured query.
    """

    unstructured_query = analyzed_result.get("unstructured_query", "")
    global_sql_query = analyzed_result.get("sql_query", "")
    original_query = analyzed_result.get("original_query", "")

    if not global_sql_query:
        raise ValueError("Query Analyzer did not return a SQL query.")

    # Initialize validator & retry handler
    validator = FederatedSQLValidator()
    retry_handler = QueryRetryHandler(max_retries=max_retries) if use_llm_retry else None

    # Validate & retry global SQL
    global_sql_valid = False
    attempt = 0
    while attempt < max_retries and not global_sql_valid:
        validation_result = validator.validate_query(global_sql_query, source_name="GLOBAL_SCHEMA")
        global_sql_valid = validation_result.get("is_valid", False)

        if global_sql_valid:
            break

        if not use_llm_retry:
            break

        # Retry via LLM
        global_sql_query = retry_handler.retry_global_sql(
            natural_query=original_query,
            previous_sql=global_sql_query,
            validation_errors=validation_result.get("errors", [])
        )
        attempt += 1

    # Translate and validate per source
    structured_queries = {}
    
    # 
    for source in GAV_MAPPINGS.keys():
        db_type = SOURCE_TO_DB_Type.get(source, "MySQL")
        dialect = "mysql" if db_type.lower() == "mysql" else "postgres"
        
        # Fetch correct target table name from config
        target_table_name = SOURCE_TO_TABLE.get(source)
        
        # [Fix] Added debug print to confirm table selection
        print(f"[DEBUG] Source: {source} -> Target Table: {target_table_name}")

        if not target_table_name:
            print(f"[WARN] No table mapping found for source {source}. Defaulting to 'jobs'.")
            target_table_name = "jobs"

        # 
        # Initialize translator with the correct target table
        translator = SQLTranslator(
            source=source,
            target_table_name=target_table_name, 
            dialect=dialect
        )

        translated_query = translator.translate_query(global_sql_query)
        attempt = 0
        valid_translation = False
        while attempt < max_retries and not valid_translation:
            source_validation = validator.validate_query(translated_query, source_name=source)
            valid_translation = source_validation.get("is_valid", False)

            if valid_translation:
                break

            if not use_llm_retry:
                break

            local_schema = validator.get_source_schema(source)
            translated_query = retry_handler.retry_translation(
                global_sql=global_sql_query,
                source_name=source,
                db_type=db_type,
                previous_translation=translated_query,
                local_schema=local_schema,
                validation_errors=source_validation.get("errors", [])
            )
            attempt += 1

        structured_queries[source] = translated_query

    # Return simplified JSON
    return {
        "structured": structured_queries,
        "unstructured": unstructured_query,
        "global_sql": global_sql_query
    }

// D:\Projects\NextMove\components\matcher\term_normalizer.py
import json
import os
import faiss
import re
import difflib
from sentence_transformers import SentenceTransformer

class TermNormalizer:
    def __init__(self):
        # Define paths relative to this file
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.ontology_path = os.path.join(base_dir, 'workspace_folder', 'artifacts', 'ontology.json')
        self.index_path = os.path.join(base_dir, 'workspace_folder', 'artifacts', 'skills.index')
        
        self.graph_neighbors = {}
        self.synonyms = {}
        self.term_list = []
        self.term_types = {} # mapping term -> type
        self.index = None
        self.model = None
        
        # Load the resources immediately
        self._load_resources()

    def _load_resources(self):
        """Loads the JSON ontology and FAISS index."""
        if os.path.exists(self.ontology_path):
            try:
                with open(self.ontology_path, 'r') as f:
                    data = json.load(f)
                    self.graph_neighbors = data.get("graph_neighbors", {})
                    self.synonyms = data.get("synonyms", {})
                    self.term_list = data.get("terms", [])
                    self.term_types = data.get("term_types", {})
            except Exception as e:
                print(f"[WARN] Ontology load failed: {e}")
        else:
            print(f"[WARN] Ontology file missing at {self.ontology_path}")

        if os.path.exists(self.index_path):
            try:
                self.index = faiss.read_index(self.index_path)
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
            except Exception as e:
                print(f"[WARN] FAISS load failed: {e}")
        else:
            print(f"[WARN] FAISS index missing at {self.index_path}")

    def _lexical_search(self, query_term, threshold=0.85):
        """
        Week 5: Lexical Matching (Sequence/Edit Distance).
        Catches typos in specific entity names (e.g., 'Flipcart' -> 'Flipkart').
        """
        if not self.term_list:
            return None
        # get_close_matches uses a variation of Ratcliff-Obershelp similarity
        matches = difflib.get_close_matches(query_term, self.term_list, n=1, cutoff=threshold)
        return matches[0] if matches else None

    def _semantic_search(self, query_term, threshold=0.7):
        """
        Week 5: Vector-based Semantic Matching.
        """
        if not self.index or not self.model: return None
        
        try:
            vec = self.model.encode([query_term])
            distances, ids = self.index.search(vec, 1)
            
            best_id = ids[0][0]
            dist = distances[0][0]
            
            if best_id != -1 and dist < threshold:
                return self.term_list[best_id]
        except Exception:
            pass
        return None

    def _get_experience_hint(self, word):
        """Rule-based matching for Experience levels."""
        word_lower = word.lower()
        rules = {
            "fresher": "0-1 years",
            "graduate": "0-1 years",
            "entry": "0-2 years",
            "junior": "1-3 years",
            "senior": "5+ years",
            "lead": "8+ years",
            "intern": "0 years"
        }
        for key, val in rules.items():
            if key in word_lower:
                return f"Experience Context: '{word}' implies '{val}'"
        return None

    def expand_query(self, natural_query: str) -> str:
        """
        Main function to analyze the query and return semantic hints.
        """
        words = natural_query.split()
        hints = []
        
        for word in words:
            clean_word = word.strip("?,.!").lower()
            
            # 1. Check Experience Rules (Rule Based)
            exp_hint = self._get_experience_hint(clean_word)
            if exp_hint:
                hints.append(exp_hint)
                continue

            # 2. Check Synonyms (Exact Match)
            matched_term = None
            if clean_word in self.synonyms:
                matched_term = self.synonyms[clean_word]
                hints.append(f"Synonym: '{word}' implies '{matched_term}'")
            
            # 3. HYBRID MATCHING (Lexical + Semantic)
            if not matched_term:
                # A. Try Lexical First (Fast, catches typos)
                matched_term = self._lexical_search(word)
                if matched_term:
                     hints.append(f"Spelling Correction: '{word}' treated as '{matched_term}'")
                
                # B. If no lexical match, try Semantic (Vector)
                else:
                    matched_term = self._semantic_search(word)

            # 4. Context Generation (Type Aware)
            if matched_term:
                # Identify Type (Location vs Role vs Company)
                t_type = self.term_types.get(matched_term, "entity")
                
                if t_type == "location":
                    hints.append(f"Location Context: '{word}' refers to city '{matched_term}'")
                elif t_type == "company":
                    hints.append(f"Company Context: '{word}' refers to '{matched_term}'")
                elif t_type in ["role", "skill"]:
                    # Look for graph neighbors (only for roles/skills)
                    if matched_term in self.graph_neighbors:
                        related = self.graph_neighbors[matched_term][:4]
                        hints.append(f"Graph Context: '{matched_term}' is associated with: {related}")

        if not hints: return ""
        return "\n[SEMANTIC HINTS FROM KNOWLEDGE GRAPH]\n" + "\n".join(list(set(hints)))

// D:\Projects\NextMove\components\learner\graph_learner.py
import os
import json
from keybert import KeyBERT

class GraphLearner:
    def __init__(self):
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.ontology_path = os.path.join(base_dir, 'workspace_folder', 'artifacts', 'ontology.json')
        self.kw_model = KeyBERT()

    def learn_from_results(self, user_query: str, db_results: dict):
        text_corpus = ""
        for source, data in db_results.items():
            if isinstance(data, list):
                for row in data:
                    text_corpus += f"{row.get('title', '')} {row.get('skills', '')} "

        if len(text_corpus) < 20: return

        try:
            keywords = self.kw_model.extract_keywords(
                text_corpus, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5
            )
            new_skills = [kw[0] for kw in keywords]
            
            if new_skills:
                self._update_graph(user_query, new_skills)
        except Exception as e:
            print(f"[LEARNER] Error: {e}")

    def _update_graph(self, role, skills):
        if not os.path.exists(self.ontology_path): return
        try:
            with open(self.ontology_path, 'r') as f:
                data = json.load(f)
            
            graph = data.get("graph_neighbors", {})
            # Use Title Case for consistency
            role_key = role.title() 
            
            if role_key not in graph: graph[role_key] = []
            
            updated = False
            for skill in skills:
                skill_key = skill.title()
                if skill_key not in graph[role_key]:
                    graph[role_key].append(skill_key)
                    updated = True
            
            if updated:
                data["graph_neighbors"] = graph
                with open(self.ontology_path, 'w') as f:
                    json.dump(data, f, indent=2)
                print(f"[LEARNER] Learned: {role_key} -> {skills}")
        except:
            pass

// D:\Projects\NextMove\scripts\build_knowledge_base.py
import json
import os
import networkx as nx
import pandas as pd
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# --- CONFIGURATION ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
WORKSPACE_DIR = os.path.join(BASE_DIR, 'workspace_folder')
INPUT_DIR = os.path.join(WORKSPACE_DIR, 'input')
ARTIFACTS_DIR = os.path.join(WORKSPACE_DIR, 'artifacts')

ONTOLOGY_PATH = os.path.join(ARTIFACTS_DIR, 'ontology.json')
INDEX_PATH = os.path.join(ARTIFACTS_DIR, 'skills.index')

# Input Files
FILE_ROLES = os.path.join(INPUT_DIR, 'roles.csv')
FILE_SKILLS = os.path.join(INPUT_DIR, 'skills.csv')
FILE_LOCATIONS = os.path.join(INPUT_DIR, 'locations.csv')
FILE_COMPANIES = os.path.join(INPUT_DIR, 'companies.csv')

def load_data_source():
    terms_metadata = {} # Maps term -> type (e.g. "Java": "skill")
    all_terms = []
    synonyms_map = {}
    graph_edges = []

    # --- HELPER: Generic CSV Loader ---
    def process_file(filepath, main_col, type_label, syn_col=None):
        if not os.path.exists(filepath):
            print(f"[WARN] Missing {filepath}")
            return
        
        try:
            df = pd.read_csv(filepath, on_bad_lines='skip')
            if main_col not in df.columns: return
            
            for _, row in df.iterrows():
                main_term = str(row[main_col]).strip()
                all_terms.append(main_term)
                terms_metadata[main_term] = type_label
                
                # Handle Synonyms
                if syn_col and syn_col in row and pd.notna(row[syn_col]):
                    for syn in str(row[syn_col]).split('|'):
                        clean_syn = syn.strip()
                        if clean_syn:
                            synonyms_map[clean_syn.lower()] = main_term
                            
                # Handle Roles <-> Skills Graph Edges (Specific to roles.csv)
                if type_label == 'role' and 'Skills' in row and pd.notna(row['Skills']):
                    for s in str(row['Skills']).split('|'):
                        s_clean = s.strip()
                        if s_clean:
                            graph_edges.append((main_term, s_clean))
                            
        except Exception as e:
            print(f"[ERROR] Failed to process {filepath}: {e}")

    # --- LOAD ALL DATA ---
    print(f"[INFO] Loading Data from {INPUT_DIR}...")
    process_file(FILE_ROLES, 'Role', 'role', 'Synonyms')
    process_file(FILE_SKILLS, 'Skill', 'skill', 'Synonyms')
    process_file(FILE_LOCATIONS, 'Location', 'location', 'Synonyms')
    process_file(FILE_COMPANIES, 'Company', 'company', 'Synonyms')

    # Fallback for Indian Context if empty
    if not all_terms:
        return generate_synthetic_data()

    return all_terms, terms_metadata, synonyms_map, graph_edges

def generate_synthetic_data():
    """Fallback with Indian Context + Locations"""
    print("[INFO] Generating Synthetic Data...")
    terms = ["Data Scientist", "Python", "Bengaluru", "Mumbai", "Google", "TCS", "Fresher", "SDE-1"]
    metadata = {
        "Data Scientist": "role", "Python": "skill", 
        "Bengaluru": "location", "Mumbai": "location",
        "Google": "company", "TCS": "company"
    }
    synonyms = {"blr": "Bengaluru", "bombay": "Mumbai", "ml": "Machine Learning"}
    edges = [("Data Scientist", "Python")]
    return terms, metadata, synonyms, edges

def build_knowledge_base():
    os.makedirs(ARTIFACTS_DIR, exist_ok=True)
    
    all_terms, terms_metadata, synonyms, edges = load_data_source()
    
    if not all_terms:
        print("[ERROR] No terms found.")
        return

    print(f"[INFO] Indexing {len(all_terms)} terms...")

    # 1. Build Vector Index
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(all_terms, show_progress_bar=True)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    faiss.write_index(index, INDEX_PATH)
    
    # 2. Build Graph
    G = nx.Graph()
    for u, v in edges:
        G.add_edge(u, v)
    
    # 3. Save Metadata
    metadata = {
        "terms": all_terms, 
        "term_types": terms_metadata, # Stores if "Python" is a skill or company
        "graph_neighbors": {n: list(G.neighbors(n)) for n in G.nodes()},
        "synonyms": synonyms
    }
    
    with open(ONTOLOGY_PATH, 'w') as f:
        json.dump(metadata, f, indent=2)
        
    print(f"[SUCCESS] Knowledge Base Built. Graph Nodes: {G.number_of_nodes()}")

if __name__ == "__main__":
    build_knowledge_base()

// D:\Projects\NextMove\components\synthesizer\integration.py
# D:\Projects\NextMove\components\synthesizer\integration.py

import difflib
import re
import random
from datetime import datetime
from typing import List, Dict, Any
from components.matcher.term_normalizer import TermNormalizer
from entities.config import GAV_MAPPINGS

class ResultIntegrator:
    def __init__(self, threshold: float = 0.85):
        self.threshold = threshold
        self.normalizer = TermNormalizer()
        
        # --- SCORING WEIGHTS ---
        self.WEIGHT_COMPANY_MATCH = 4.0  
        self.WEIGHT_TITLE_MATCH = 3.0
        self.WEIGHT_SKILL_MATCH = 2.0
        self.WEIGHT_DESC_MATCH = 1.0
        self.WEIGHT_LOCATION_MATCH = 2.0
        self.WEIGHT_RECENCY = 2.5
        self.WEIGHT_SEMANTIC_BONUS = 0.5

        # Stopwords
        self.STOPWORDS = {
            "are", "there", "any", "jobs", "available", "at", "in", "for", 
            "opening", "role", "position", "work", "vacancy", "hiring", "job", 
            "give", "show", "me", "list", "find"
        }

    # ==========================================
    # 0. NORMALIZATION & CLEANING
    # ==========================================
    def _standardize_job(self, raw_job: Dict, source: str) -> Dict:
        """Maps source keys to Global Schema keys."""
        standardized_job = {}
        mapping = GAV_MAPPINGS.get(source, {})

        for global_key, local_key in mapping.items():
            # 1. Try mapped key
            if local_key and local_key in raw_job:
                standardized_job[global_key] = raw_job[local_key]
            # 2. Try direct key match (e.g., from SQL aliases)
            elif global_key in raw_job:
                standardized_job[global_key] = raw_job[global_key]
            else:
                standardized_job[global_key] = None

        standardized_job['_source'] = source
        return standardized_job

    def _prune_job(self, job: Dict) -> Dict:
        """
        Final cleanup: 
        - Removes keys with None/Empty values (Optimization).
        - Removes internal scoring keys (Noise reduction).
        """
        pruned = {}
        for k, v in job.items():
            # Remove internal ranking details
            if k.startswith("_relevance"):
                continue
            
            # Remove empty data
            if v is None or v == "" or str(v).lower() == "null":
                continue
                
            pruned[k] = v
        return pruned

    # ==========================================
    # 1. DEDUPLICATION UTILS
    # ==========================================
    def _normalize_text(self, text: Any) -> str:
        if not text: return ""
        return str(text).lower().strip()

    def _normalize_company(self, text: Any) -> str:
        s = self._normalize_text(text)
        s = re.sub(r'\b(pvt|ltd|inc|corp|llc|private|limited)\b', '', s)
        return s.strip()

    def _is_synonym(self, term_a: str, term_b: str) -> bool:
        if self.normalizer.synonyms.get(term_a) == term_b: return True
        if self.normalizer.synonyms.get(term_b) == term_a: return True
        return False

    def _is_match(self, job_a: Dict, job_b: Dict) -> bool:
        """Determines if two jobs are the same entity."""
        title_a = self._normalize_text(job_a.get('title'))
        title_b = self._normalize_text(job_b.get('title'))
        comp_a = self._normalize_company(job_a.get('company_name'))
        comp_b = self._normalize_company(job_b.get('company_name'))

        if not comp_a or not comp_b: return False
        
        # Blocking on Company Name
        if comp_a == comp_b:
            comp_sim = 1.0
        else:
            comp_sim = difflib.SequenceMatcher(None, comp_a, comp_b).ratio()
        
        if comp_sim < 0.85: return False

        # Matching on Title
        if title_a == title_b: return True
        if self._is_synonym(title_a, title_b): return True

        title_sim = difflib.SequenceMatcher(None, title_a, title_b).ratio()
        return title_sim >= self.threshold

    def _merge_jobs(self, existing_job: Dict, new_job: Dict) -> Dict:
        """Merges attributes to create a Golden Record."""
        merged = existing_job.copy()
        
        # Merge strategy: Prefer existing non-empty values, else take new
        fields = [
            'salary_range', 'skills', 'description', 'location', 
            'job_posting_date', 'experience_required', 'work_type'
        ]

        for field in fields:
            val_exist = str(existing_job.get(field, '') or '')
            val_new = str(new_job.get(field, '') or '')
            
            if (not val_exist or val_exist.lower() == 'none') and (val_new and val_new.lower() != 'none'):
                merged[field] = new_job[field]
            elif len(val_new) > len(val_exist):
                merged[field] = new_job[field]

        # Merge Source Tag
        if '_source' in existing_job and '_source' in new_job:
            if new_job['_source'] not in existing_job['_source']:
                merged['_source'] = f"{existing_job['_source']}, {new_job['_source']}"
        
        return merged

    # ==========================================
    # 2. RANKING LOGIC
    # ==========================================
    def _calculate_date_score(self, date_val: Any) -> float:
        if not date_val or str(date_val).lower() == 'none': return 0.0
        try:
            date_obj = None
            if isinstance(date_val, datetime):
                date_obj = date_val
            elif isinstance(date_val, str):
                date_str = date_val.replace("Z", "").split(".")[0]
                if "T" in date_str: 
                    date_obj = datetime.fromisoformat(date_str)
                else: 
                    try:
                        date_obj = datetime.strptime(date_str[:10], "%Y-%m-%d")
                    except:
                        return 0.5 

            if date_obj:
                if date_obj.tzinfo: date_obj = date_obj.replace(tzinfo=None)
                days_old = (datetime.now() - date_obj).days
                if days_old < 0: days_old = 0
                return 1.0 / (days_old + 1)
                
            return 0.5
        except Exception:
            return 0.5

    def _calculate_context_score(self, text: Any, direct_keywords: List[str], semantic_neighbors: List[str]) -> float:
        if not text: return 0.0
        text_lower = str(text).lower()
        score = 0.0
        
        for kw in direct_keywords:
            matches = len(re.findall(r'\b' + re.escape(kw) + r'\b', text_lower))
            score += min(matches, 3) * 1.0 
        
        for neighbor in semantic_neighbors:
            if re.search(r'\b' + re.escape(neighbor) + r'\b', text_lower):
                score += self.WEIGHT_SEMANTIC_BONUS
        return score

    def _score_job(self, job: Dict, direct_keywords: List[str], semantic_neighbors: List[str]) -> float:
        # 1. Keyword Matching
        s_company = self._calculate_context_score(job.get('company_name'), direct_keywords, semantic_neighbors)
        s_title = self._calculate_context_score(job.get('title'), direct_keywords, semantic_neighbors)
        s_skills = self._calculate_context_score(job.get('skills'), direct_keywords, semantic_neighbors)
        s_desc = self._calculate_context_score(job.get('description'), direct_keywords, semantic_neighbors)
        s_loc = self._calculate_context_score(job.get('location'), direct_keywords, semantic_neighbors)

        semantic_score = (
            (s_company * self.WEIGHT_COMPANY_MATCH) +
            (s_title * self.WEIGHT_TITLE_MATCH) +
            (s_skills * self.WEIGHT_SKILL_MATCH) +
            (s_desc * self.WEIGHT_DESC_MATCH) +
            (s_loc * self.WEIGHT_LOCATION_MATCH)
        )

        # 2. Recency
        recency_score = self._calculate_date_score(job.get('job_posting_date'))
        
        # 3. Completeness
        completeness = 0.0
        if job.get('salary_range'): completeness += 0.2
        if job.get('work_type'): completeness += 0.1

        total = semantic_score + (recency_score * self.WEIGHT_RECENCY) + completeness
        return total + random.uniform(0, 0.01)

    def _get_semantic_expansion(self, user_intent: str) -> tuple[List[str], List[str]]:
        raw_words = re.findall(r'\w+', user_intent.lower())
        direct_keywords = []
        neighbors = set()

        for word in raw_words:
            if word in self.STOPWORDS or len(word) < 2: continue
            direct_keywords.append(word)
            if word in self.normalizer.synonyms:
                neighbors.add(self.normalizer.synonyms[word])
            
            word_cap = word.title()
            if word_cap in self.normalizer.graph_neighbors:
                related_terms = self.normalizer.graph_neighbors[word_cap]
                for term in related_terms[:3]:
                    neighbors.add(term.lower())

        return direct_keywords, list(neighbors)

    # ==========================================
    # MAIN ENTRY POINT
    # ==========================================
    def integrate_and_rank(self, results_dict: Dict[str, List[Dict]], user_intent: str, limit: int = 10) -> List[Dict]:
        direct_keywords, semantic_neighbors = self._get_semantic_expansion(user_intent)
        
        seen_jobs = [] 

        # Step 1: Standardize & Deduplicate
        for source, jobs in results_dict.items():
            if not isinstance(jobs, list): continue
            
            for raw_job in jobs:
                if not isinstance(raw_job, dict): continue
                
                # 1. Standardize Keys
                std_job = self._standardize_job(raw_job, source)
                
                # 2. Deduplicate
                is_duplicate = False
                for i, existing_job in enumerate(seen_jobs):
                    if self._is_match(std_job, existing_job):
                        seen_jobs[i] = self._merge_jobs(existing_job, std_job)
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    seen_jobs.append(std_job)

        # Step 2: Rank
        for job in seen_jobs:
            relevance = self._score_job(job, direct_keywords, semantic_neighbors)
            job['_relevance_score'] = round(relevance, 3)

        # Sort desc
        seen_jobs.sort(key=lambda x: x['_relevance_score'], reverse=True)

        # Step 3: Prune & Limit (Clean up for LLM)
        final_jobs = []
        for job in seen_jobs[:limit]:
            final_jobs.append(self._prune_job(job))

        return final_jobs

